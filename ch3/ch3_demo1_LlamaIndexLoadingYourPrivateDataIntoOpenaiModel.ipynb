{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7764a54",
   "metadata": {
    "id": "a7764a54",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Demo - RAG with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf90779",
   "metadata": {
    "id": "dbf90779"
   },
   "source": [
    "Demo of how to use LlamaIndex for reading files from local into OpenAI model and ask questions related to you data. The purpose of this demo is to show how OpenAI model can not have information about your private data to be able to give answers. LlamaIndex can help to integrate and connect your private data with OpenAI models using RAG and different types of document indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a194b",
   "metadata": {
    "id": "615a194b"
   },
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7ffc6e",
   "metadata": {
    "id": "ac7ffc6e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index --quiet\n",
    "%pip install openai --quiet\n",
    "%pip install pip install docx2txt --quiet\n",
    "%pip install llama-index openai pypdf --quiet\n",
    "# using s3fs to get public bucket contents\n",
    "%pip install fs_s3fs --quiet\n",
    "%pip install s3fs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i2uSonYeYjQ9",
   "metadata": {
    "id": "i2uSonYeYjQ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# %pip install awscli --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcd82b",
   "metadata": {
    "id": "cafcd82b"
   },
   "source": [
    "Set up the OpenAI API key as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64bc281d",
   "metadata": {
    "id": "64bc281d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import s3fs # so we can access public s3 buckets\n",
    "\n",
    "api_key = \"\"\n",
    "#fix\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "#old\n",
    "#openai.api_key = api_key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa68c6",
   "metadata": {
    "id": "78fa68c6"
   },
   "source": [
    "Download Private-Data locally using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a715dc4d",
   "metadata": {
    "id": "a715dc4d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-30 21:02:08--  https://btcampdata.s3.amazonaws.com/Private-Data.zip\n",
      "Resolving btcampdata.s3.amazonaws.com (btcampdata.s3.amazonaws.com)... 52.219.102.35, 52.219.107.28, 52.219.106.204, ...\n",
      "Connecting to btcampdata.s3.amazonaws.com (btcampdata.s3.amazonaws.com)|52.219.102.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 96732 (94K) [application/zip]\n",
      "Saving to: ‘Private-Data.zip’\n",
      "\n",
      "Private-Data.zip    100%[===================>]  94.46K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2023-11-30 21:02:08 (1.01 MB/s) - ‘Private-Data.zip’ saved [96732/96732]\n",
      "\n",
      "Archive:  Private-Data.zip\n",
      "  inflating: Private-Data/CV2.pdf    \n",
      "  inflating: Private-Data/CV1.pdf    \n"
     ]
    }
   ],
   "source": [
    "#old\n",
    "# ! aws s3 cp s3://webage-genai-data/Private-Data/ Private-Data --recursive\n",
    "\n",
    "!wget https://btcampdata.s3.amazonaws.com/Private-Data.zip\n",
    "!unzip Private-Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb6d33",
   "metadata": {
    "id": "0bdb6d33"
   },
   "source": [
    "## Check OpenAI model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecb53c",
   "metadata": {
    "id": "ffecb53c"
   },
   "source": [
    "Check how the OpenAI model would answer on some specific question related to our private database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92541acd",
   "metadata": {
    "id": "92541acd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                            messages=messages,\n",
    "                                            temperature=0)\n",
    "\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22576a6",
   "metadata": {
    "id": "e22576a6"
   },
   "source": [
    "Our question that we will ask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed20bf0e",
   "metadata": {
    "id": "ed20bf0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"For what companies did Susan work in the past?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a133020",
   "metadata": {
    "id": "3a133020",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I'm sorry, but as an AI language model, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I can't provide information about Susan's past work or any other personal details unless they have been explicitly provided to me.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07ff9f",
   "metadata": {
    "id": "ba07ff9f"
   },
   "source": [
    "**We see that ChatGPT does not have information about Susan at all.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9fa27",
   "metadata": {
    "id": "3ee9fa27"
   },
   "source": [
    "## Connect custom data sources to your LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80efd0",
   "metadata": {
    "id": "5c80efd0"
   },
   "source": [
    "We can use RAG to load our own data and feed LLM with our documents as context.\n",
    "\n",
    "It comes with many ready-made readers for sources such as databases, Discord, Slack, Google Docs, Notion, GitHub reps etc.\n",
    "\n",
    "Full list can be found here: https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d2e5f",
   "metadata": {
    "id": "563d2e5f"
   },
   "source": [
    "**SimpleDirectoryReader**\n",
    "\n",
    "`SimpleDirectoryReader` is used for reading data locally.\n",
    "In order to use it, simply pass in a input directory or a list of files. In our **Private-Data** folder, we have couple of CVs pdf files.\n",
    "\n",
    "`SimpleDirectoryReader` will select the best file reader (either for csv, pdf etc) based on the file extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1198",
   "metadata": {
    "id": "fdba1198"
   },
   "source": [
    "Load CV files using SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18eff84",
   "metadata": {
    "id": "b18eff84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import TreeIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('Private-Data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a953503",
   "metadata": {
    "id": "2a953503"
   },
   "source": [
    "By default, LlamaIndex uses OpenAI GPT-3 **text-davinci-003** model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b33361",
   "metadata": {
    "id": "62b33361"
   },
   "source": [
    "Create **TreeIndex** using loaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4097a0ad",
   "metadata": {
    "id": "4097a0ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = TreeIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28451da0",
   "metadata": {
    "id": "28451da0"
   },
   "source": [
    "Creating query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966d6526",
   "metadata": {
    "id": "966d6526",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd08d8",
   "metadata": {
    "id": "06cd08d8"
   },
   "source": [
    "Getting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b81e8e",
   "metadata": {
    "id": "16b81e8e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for Luxury Car Center and Japan Car Center in the past.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"For what companies did Susan work in the past?\"\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf94ab",
   "metadata": {
    "id": "10bf94ab"
   },
   "source": [
    "Depending on the answer, but if the model did not know, we might try different type if indexing or changing OpenAI model. Or change prompt to be more specific.\n",
    "\n",
    "Let's try new OpenAI model firstly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a2211",
   "metadata": {
    "id": "877a2211"
   },
   "source": [
    "### Changing the underlying LLM / Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2cd46",
   "metadata": {
    "id": "efd2cd46"
   },
   "source": [
    "Sometimes, you want to use some other LLM for indexing instead of the default one.\n",
    "\n",
    "In this example, we use **gpt-3.5-turbo** instead of **text-davinci-003**. Available models include, gpt-3.5-turbo-16k, gpt-4, gpt-4-32k, text-davinci-003, and text-davinci-002 and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49662a34",
   "metadata": {
    "id": "49662a34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8966bc",
   "metadata": {
    "id": "1f8966bc"
   },
   "source": [
    "Define a new LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd4e89f",
   "metadata": {
    "id": "8bd4e89f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-16k-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dea4c1",
   "metadata": {
    "id": "45dea4c1"
   },
   "source": [
    "Create **contex_service**\n",
    "\n",
    "The ServiceContext is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. You can use it to set the global configuration, as well as local configurations at specific parts of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154968f8",
   "metadata": {
    "id": "154968f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7604ac5",
   "metadata": {
    "id": "c7604ac5"
   },
   "source": [
    "Create again **TreeIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969707bc",
   "metadata": {
    "id": "969707bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import TreeIndex\n",
    "\n",
    "index = TreeIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3abc01",
   "metadata": {
    "id": "7f3abc01"
   },
   "source": [
    "Start query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6161705a",
   "metadata": {
    "id": "6161705a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24574a",
   "metadata": {
    "id": "bd24574a"
   },
   "source": [
    "Run the same prompt as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b041ee",
   "metadata": {
    "id": "f2b041ee",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for Luxury Car Center and Japan Car Center in the past.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb39d3b",
   "metadata": {
    "id": "ceb39d3b"
   },
   "source": [
    "Let's try another type of indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bac38cb",
   "metadata": {
    "id": "7bac38cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bbf5e9b",
   "metadata": {
    "id": "1bbf5e9b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for LUXURY CAR CENTER and JAPAN CAR CENTER in the past.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc6223",
   "metadata": {
    "id": "84bc6223"
   },
   "source": [
    "If the model still does not know the answer, we can be more specific with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6bc5a4f",
   "metadata": {
    "id": "a6bc5a4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"For what companies did Susan work in the past?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e0f3384",
   "metadata": {
    "id": "1e0f3384",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for LUXURY CAR CENTER and JAPAN CAR CENTER in the past.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f514",
   "metadata": {
    "id": "5910f514"
   },
   "source": [
    "### Try another prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3f1cca7",
   "metadata": {
    "id": "c3f1cca7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt2 = \"Give me two names for software engineering position?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a027b9c7",
   "metadata": {
    "id": "a027b9c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher Morgan, Senior Web Developer\n",
      "Susan Williams, Store Manager\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588761a1",
   "metadata": {
    "id": "588761a1"
   },
   "source": [
    "### Let's try another type of indexing to see the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605f19e",
   "metadata": {
    "id": "b605f19e"
   },
   "source": [
    "### GPTVectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05c0b2",
   "metadata": {
    "id": "ca05c0b2"
   },
   "source": [
    "GPTVectorStoreIndex creates numerical vectors from the text using word embeddings and retrieves relevant documents based on the similarity of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a82e7b2",
   "metadata": {
    "id": "4a82e7b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index2 = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine2 = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ff7bb8",
   "metadata": {
    "id": "69ff7bb8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for LUXURY CAR CENTER and JAPAN CAR CENTER in the past.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine2.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6c12e4c",
   "metadata": {
    "id": "e6c12e4c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher Morgan, Senior Web Developer\n",
      "Susan Williams, Store Manager\n"
     ]
    }
   ],
   "source": [
    "response = query_engine2.query(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d28e0f",
   "metadata": {
    "id": "86d28e0f"
   },
   "source": [
    "### GPTListIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0471eb",
   "metadata": {
    "id": "ae0471eb"
   },
   "source": [
    "The GPTListIndex index is perfect when you don’t have many documents. Instead of trying to find the relevant data, the index concatenates all chunks and sends them all to the LLM. If the resulting text is too long, the index splits the text and asks LLM to refine the answer.\n",
    "\n",
    "Since we do not have too much documents, let's see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55e3ce93",
   "metadata": {
    "id": "55e3ce93",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.list import GPTListIndex\n",
    "\n",
    "index3 = GPTListIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine3 = index3.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24ad091b",
   "metadata": {
    "id": "24ad091b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for Luxury Car Center and Japan Car Center in the past.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine3.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee744e4a",
   "metadata": {
    "id": "ee744e4a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher Morgan\n",
      "Susan Williams\n"
     ]
    }
   ],
   "source": [
    "response = query_engine3.query(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41bd9c",
   "metadata": {
    "id": "ef41bd9c"
   },
   "source": [
    "### GPTKeywordTableIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45036fef",
   "metadata": {
    "id": "45036fef"
   },
   "source": [
    "The GPTKeywordTableIndex implementation extracts the keywords from indexed nodes and uses them to find relevant documents. When we ask a question, first, the implementation will generate keywords from the question. Next, the index searches for the relevant documents and sends them to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec3303",
   "metadata": {
    "id": "75ec3303"
   },
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Using Keyword Indexing, every node is sent to the LLM to generate keywords. Sending every document to an LLM skyrockets the cost of indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "280c3e77",
   "metadata": {
    "id": "280c3e77",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.keyword_table import GPTKeywordTableIndex\n",
    "\n",
    "index4 = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine4 = index4.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8d8e79e",
   "metadata": {
    "id": "a8d8e79e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "response = query_engine4.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f5f95f6",
   "metadata": {
    "id": "7f5f95f6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine4.query(prompt2)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe98afd",
   "metadata": {
    "id": "6fe98afd"
   },
   "source": [
    "### Saving and Loading indexed documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1631803",
   "metadata": {
    "id": "f1631803"
   },
   "source": [
    "By default, data is stored in-memory. To persist to disk (under ./storage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce1f3b",
   "metadata": {
    "id": "97ce1f3b"
   },
   "outputs": [],
   "source": [
    "index4.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bea4e",
   "metadata": {
    "id": "929bea4e"
   },
   "source": [
    "To reload from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92de84",
   "metadata": {
    "id": "4c92de84"
   },
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# load index\n",
    "loaded_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10932ac2",
   "metadata": {
    "id": "10932ac2"
   },
   "outputs": [],
   "source": [
    "query_engine_loaded = loaded_index.as_query_engine()\n",
    "\n",
    "\n",
    "response_loaded_index = query_engine_loaded.query(prompt2)\n",
    "print(response_loaded_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TfMpDxfUU33-",
   "metadata": {
    "id": "TfMpDxfUU33-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
